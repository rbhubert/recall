.\" Man page generated from reStructuredText.
.
.TH TINYSEGMENTER 7 "2012-09-24" "0.2" ""
.SH NAME
tinysegmenter \- Python module implementing a Japanese tokenizer.
.
.nr rst2man-indent-level 0
.
.de1 rstReportMargin
\\$1 \\n[an-margin]
level \\n[rst2man-indent-level]
level margin: \\n[rst2man-indent\\n[rst2man-indent-level]]
-
\\n[rst2man-indent0]
\\n[rst2man-indent1]
\\n[rst2man-indent2]
..
.de1 INDENT
.\" .rstReportMargin pre:
. RS \\$1
. nr rst2man-indent\\n[rst2man-indent-level] \\n[an-margin]
. nr rst2man-indent-level +1
.\" .rstReportMargin post:
..
.de UNINDENT
. RE
.\" indent \\n[an-margin]
.\" old: \\n[rst2man-indent\\n[rst2man-indent-level]]
.nr rst2man-indent-level -1
.\" new: \\n[rst2man-indent\\n[rst2man-indent-level]]
.in \\n[rst2man-indent\\n[rst2man-indent-level]]u
..
.SH SYNOPSIS
.sp
import tinysegmenter
.SH DESCRIPTION
.sp
\fBtinysegmenter\fP is an extremely compact Japanese segmenter written natively in Python (supports Python 2 and 3).
It contains a unique class \fBTinySegmenter\fP which in turns defines a single "public" method \fBtokenize()\fP\&.
.sp
The \fBtokenize(text)\fP call takes a natural language Japanese text as a unicode string parameter \fBtext\fP,
and returns a list of unicode strings, each element of the list being a lexical token of the initial parameter.
.sp
Interface:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
class tinysegmenter.TinySegmenter:
    def tokenize(ustring): # Return type list(ustring)
.ft P
.fi
.UNINDENT
.UNINDENT
.SH APPLICATION USAGE
.sp
This module allows to simply break Japanese sentences into lexical tokens,
which in natural language could be considered as grammatical words.
This natural language processing (NLP) is particular for Japanese,
as this agglutinative does not use any space, and very few punctuations, making it
particularly difficult to tokenize a sentence.
.SH EXAMPLE
.sp
The sentence "I live in Tokyo" being written "東京で住んにいます", here is how you could tokenize it:
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import tinysegmenter
segmenter = tinysegmenter.TinySegmenter()
tokens = segmenter.tokenize(u\(aq東京に住む\(aq)
.ft P
.fi
.UNINDENT
.UNINDENT
.sp
\fBtokens\fP would now hold the value: [u\(aq東京\(aq, u\(aqに\(aq, u\(aq住む\(aq] (\(aqTokyo\(aq, location particle, and verb).
.SH COMPATIBILITY
.sp
\fBtinysegmenter.TinySegmenter\fP ‘s interface is compatible with the \fINatural Language Toolkit\fP (\fBNLTK\fP) python module’s \fBTokenizerI\fP class,
although the distribution does not directly depend on NLTK.
Here is one way to use it as a tokenizer in NLTK (order of the multiple base classes matters):
.INDENT 0.0
.INDENT 3.5
.sp
.nf
.ft C
import nltk.tokenize.api
class myTinySegmenter(tinysegmenter.TinySegmenter, nltk.tokenize.api.TokenizerI):
    pass
segmenter = myTinySegmenter()
# This segmenter can be used any place which expects a NLTK\(aqs TokenizerI subclass.
.ft P
.fi
.UNINDENT
.UNINDENT
.SH SEE ALSO
.sp
TinySegmenter\(aqs currently maintained website can be found here: \fI\%http://tinysegmenter.tuxfamily.org/\fP
.sp
It is available also as a pypi module: \fI\%http://pypi.python.org/pypi/tinysegmenter\fP
.sp
All bug, patch, question, etc. can be sent to \fBtinysegmenter at zemarmot dot net\fP\&.
.sp
To know more about NLTK\(aqs \fBTokenizerI\fP, and NLTK itself, see: \fI\%http://nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI\fP
.SH ABOUT
.sp
Copyright (c) 2008, Taku Kudo
Copyright (c) 2010, Masato Hagiwara
Copyright (c) 2012, Jehan
.sp
This module is distributed under a \fBNew BSD\fP license that you should find in this distribution.
.SH AUTHOR
tinysegmenter at zemarmot.net
.\" Generated by docutils manpage writer.
.
